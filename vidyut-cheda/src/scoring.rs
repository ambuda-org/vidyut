//! Scores a tagged Sanskrit sentence.

// `modular_bitfield` generates some methods that we don't use. Clippy warns us about these unused
// methods, but since these methods are autogenerated, I don't know of a good way to mark them as
// allowed. So instead, just allow dead code in this module.
#![allow(dead_code)]

use crate::errors::{Error, Result};
use crate::segmenting::{Phrase, TokenPool};
use core::str::FromStr;
use modular_bitfield::prelude::*;
use rustc_hash::FxHashMap;
use std::path::Path;
use vidyut_kosha::morph::*;
use vidyut_kosha::packing::{PackedLinga, PackedVacana, PackedVibhakti};

/// Models a Markov transition state.
#[derive(Copy, Clone, Default, Eq, PartialEq, Hash)]
pub struct State(u16);

impl State {
    /// Creates an empty transition state.
    pub fn new() -> Self {
        PadaState::initial_state().into_state()
    }

    /// Initializes a transition state from the given pada.
    pub fn from_pada(s: &Pada) -> Self {
        PadaState::from_pada(s).into_state()
    }
}

impl ToString for State {
    fn to_string(&self) -> String {
        self.0.to_string()
    }
}

impl FromStr for State {
    type Err = Error;
    fn from_str(s: &str) -> Result<Self> {
        Ok(Self(s.parse()?))
    }
}

/// Models the transition state for some *subanta*.
///
/// The state describes linga, vibhakti, and vacana, which are sufficient for our current needs.
#[bitfield(bits = 14)]
pub struct SubantaState {
    #[skip(getters)]
    unused: B5,
    #[skip(getters)]
    linga: PackedLinga,
    #[skip(getters)]
    vacana: PackedVacana,
    #[skip(getters)]
    vibhakti: PackedVibhakti,
    #[skip(getters)]
    is_purvapada: bool,
}

/// Models the transition state for some *tinanta*.
///
/// The state describes purusha and vacana, which are sufficient for our current needs.
#[bitfield(bits = 14)]
pub struct TinantaState {
    #[skip(getters)]
    unused: B10,
    #[skip(getters)]
    purusha: Purusha,
    #[skip(getters)]
    vacana: Vacana,
}

/// Models the transition state for some *pada*.
#[bitfield(bits = 16)]
pub struct PadaState {
    #[skip(getters)]
    pos: POSTag,
    #[skip(getters)]
    payload: B14,
}

impl PadaState {
    /// Creates the initial state.
    pub fn initial_state() -> Self {
        PadaState::new().with_pos(POSTag::Unknown)
    }

    /// Creates a state label for the given pada.
    pub fn from_pada(p: &Pada) -> Self {
        let zero = [0_u8; 2];
        let (pos_tag, payload) = match p {
            Pada::Unknown => (POSTag::Unknown, zero),
            Pada::Subanta(s) => {
                let bytes = SubantaState::new()
                    .with_linga(s.linga.into())
                    .with_vacana(s.vacana.into())
                    .with_vibhakti(s.vibhakti.into())
                    .with_is_purvapada(s.is_purvapada)
                    .into_bytes();
                (POSTag::Subanta, bytes)
            }
            Pada::Tinanta(s) => {
                let bytes = TinantaState::new()
                    .with_purusha(s.purusha)
                    .with_vacana(s.vacana)
                    .into_bytes();
                (POSTag::Tinanta, bytes)
            }
            Pada::Avyaya(_) => (POSTag::Avyaya, zero),
        };
        PadaState::new()
            .with_pos(pos_tag)
            .with_payload(u16::from_le_bytes(payload))
    }

    /// Packs the given semantics into a `State`.
    pub fn into_state(self) -> State {
        let val = u16::from_le_bytes(self.into_bytes());
        State(val)
    }
}

/// Calculates the log probability given some numerator and denominator.
fn log_prob(num: f64, denom: i32) -> f32 {
    let prob = num / f64::from(denom);
    prob.log10() as f32
}

type LemmaMap<T> = FxHashMap<(usize, POSTag), T>;

struct LemmaModel {
    strings: FxHashMap<String, usize>,
    /// Log probability that a (lemma, pos) appears.
    log_probs: FxHashMap<(usize, POSTag), f32>,
    /// The log probability of a token not seen anywhere in the training data.
    log_p_unknown: f32,
}

impl LemmaModel {
    fn new(path: impl AsRef<Path>) -> Result<Self> {
        let mut counts = FxHashMap::default();
        let mut strings = FxHashMap::default();

        let mut rdr = csv::Reader::from_path(path)?;
        for maybe_row in rdr.records() {
            let r = maybe_row?;
            let lemma = &r[0];
            let pos_tag = r[1].parse()?;
            let count = r[2].parse::<i32>()?;

            let i = match strings.get(lemma) {
                Some(i) => *i,
                None => {
                    let i = strings.len();
                    strings.insert(lemma.to_string(), i);
                    i
                }
            };
            counts.insert((i, pos_tag), count);
        }

        let (log_probs, log_p_unknown) = Self::to_log_probs(counts);
        Ok(Self {
            strings,
            log_probs,
            log_p_unknown,
        })
    }

    fn to_log_probs(counts: LemmaMap<i32>) -> (LemmaMap<f32>, f32) {
        // Use a very small smoothing factor because most out-of-vocabulary tokens are errors.
        let eps: f64 = 1e-100;

        let n: i32 = counts.values().sum();
        let num_keys = counts.len() as i32;

        let log_probs: FxHashMap<_, f32> = counts
            .iter()
            .map(|(k, c)| {
                let smoothed_v = log_prob(f64::from(*c) + eps, n + num_keys);
                (k.clone(), smoothed_v)
            })
            .collect();

        let log_p_epsilon = log_prob(eps, n + num_keys);

        (log_probs, log_p_epsilon)
    }

    fn log_prob(&self, lemma: &str, pos_tag: POSTag) -> f32 {
        if let Some(i) = self.strings.get(lemma) {
            if let Some(log_prob) = self.log_probs.get(&(*i, pos_tag)) {
                return *log_prob;
            }
        }
        self.log_p_unknown
    }
}

struct TransitionModel {
    /// Log probability of `(state[n] | state[n-1])`
    log_probs: FxHashMap<(State, State), f32>,
    /// The log probability of a transition state not seen anywhere in the training data.
    log_epsilon: f32,
}

impl TransitionModel {
    fn new(path: impl AsRef<Path>) -> Result<Self> {
        type Key = (State, State);

        let mut log_probs = FxHashMap::default();
        let mut rdr = csv::Reader::from_path(path)?;
        for maybe_row in rdr.records() {
            let row = maybe_row?;

            let key: Key = (row[0].parse()?, row[1].parse()?);
            let prob = row[2].parse::<f32>()?;

            log_probs.insert(key, prob.log10());
        }

        Ok(Self {
            log_probs,
            // FIXME: calculate this properly.
            log_epsilon: -5.0,
        })
    }

    fn log_prob(&self, prev: &State, cur: &State) -> f32 {
        let key = (*prev, *cur);
        match self.log_probs.get(&key) {
            Some(s) => *s,
            None => self.log_epsilon,
        }
    }
}

/// A simple statistical model.
pub struct Model {
    lemmas: LemmaModel,
    transitions: TransitionModel,
}

impl Model {
    /// Loads a new model from the given paths.
    pub fn new<P: AsRef<Path>>(lemma_counts_path: P, transitions_path: P) -> Result<Self> {
        let lemmas = LemmaModel::new(&lemma_counts_path)?;
        let transitions = TransitionModel::new(&transitions_path)?;

        Ok(Model {
            lemmas,
            transitions,
        })
    }

    /// Scores the given phrase by using lemma probabilities.
    ///
    /// We return our float score as an i32 because floats aren't hashed by default in Rust. To
    /// represent "floatness," multiply the float score by 100 so that the ones and tens places
    /// represent the tenths and hundredths places, respectively.
    pub(crate) fn score(&self, phrase: &Phrase, pool: &TokenPool) -> i32 {
        let n = phrase.tokens.len();
        let delta = if let Some(i_last) = phrase.tokens.last() {
            let prev_state = if n >= 2 {
                let i = phrase.tokens[n - 2];
                State::from_pada(&pool.get(i).expect("present").info)
            } else {
                State::new()
            };

            let last = pool.get(*i_last).expect("present");
            let cur_state = State::from_pada(&last.info);

            let pada = &last.info;
            let lemma_log_prob = self
                .lemmas
                .log_prob(pada.lemma(), pada.part_of_speech_tag());
            let transition_log_prob = self.transitions.log_prob(&prev_state, &cur_state);
            lemma_log_prob + transition_log_prob
        } else {
            0.0
        };

        // To simplify the scoring, assume that:
        //
        //     P(W[0], ..., W[n]) = P(W[0], ..., W[n-1]) * P(W[n] | W[0], ..., W[n-1])
        phrase.score + (100_f32 * delta) as i32
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_log_prob() {
        assert_eq!(log_prob(10.0, 10), 0.0);
        assert_eq!(log_prob(10.0, 100), -1.0);
        assert_eq!(log_prob(10.0, 1000), -2.0);
    }
}
